{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea75505d",
   "metadata": {},
   "source": [
    "# CSCN8020 – Assignment 1: Reinforcement Learning Programming\n",
    "**Student:** Babandeep  \n",
    "\n",
    "This notebook implements **all four problems** step by step:\n",
    "\n",
    "1) **Pick-and-Place Robot** – MDP design (theory, clearly reasoned).  \n",
    "2) **2×2 Gridworld** – Two iterations of Value Iteration (manual math, with policy improvement noted).  \n",
    "3) **5×5 Gridworld** – Value Iteration (standard & in-place), reward function from assignment, optimal V* & π*, and performance comparison.  \n",
    "4) **Off-policy Monte Carlo (Importance Sampling)** – Model-free estimate of V(s) using a random behavior policy and a greedy target policy; comparison with Value Iteration.\n",
    "\n",
    "> **How to run:** Execute cells top-to-bottom. Only `numpy` and `matplotlib` are used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37d7eef",
   "metadata": {},
   "source": [
    "## Problem 1 — Pick-and-Place Robot: MDP Design (Theory)\n",
    "\n",
    "**Goal:** Learn **fast** and **smooth** arm motions for a repetitive pick-and-place task by directly controlling motors (low-level control).\n",
    "\n",
    "### Step 1: Define the State Space  \\(S\\)\n",
    "Choose variables that make the process **Markov** (future depends only on current state and action):\n",
    "- Joint positions **q ∈ ℝⁿ** and joint velocities **q̇ ∈ ℝⁿ** (for an n-DoF arm).\n",
    "- Gripper state **g ∈ {0,1}** (open/closed).\n",
    "- Object pose (e.g., **pₒ = (xₒ, yₒ, zₒ, θₒ)**) and Goal pose **p_g = (x_g, y_g, z_g, θ_g)**.\n",
    "- (Optional but useful) Force/torque or contact signals; previous action **a_{t−1}** (to penalize action jumps).\n",
    "\n",
    "**Minimal Markov state (one practical choice):**  \n",
    "sₜ = [qₜ, q̇ₜ, gₜ, pₒ, p_g]\n",
    "\n",
    "### Step 2: Define the Action Space  \\(A\\)\n",
    "Low-level, continuous control (choose one consistent mode):\n",
    "- **Joint torques** **τ ∈ ℝⁿ**  *(or)*  **joint velocities** *(or)* **joint position setpoints**.\n",
    "> We assume **joint torques τ** (most direct for “fast & smooth”).\n",
    "\n",
    "### Step 3: Transitions  \\(P(s'|s,a)\\)\n",
    "Robot physics with small noise: mostly deterministic in simulation. Invalid configurations are avoided by the physics engine or handled as failure.\n",
    "\n",
    "### Step 4: Reward Function  \\(R(s,a,s')\\)\n",
    "Shape reward to achieve **task success**, **speed**, **smoothness**, and **safety**:\n",
    "- **Success:** +r_goal (e.g., +100) when object placed within tolerance at goal.\n",
    "- **Time penalty:** −c_time per step (fewer steps ⇒ faster).\n",
    "- **Smoothness/effort:** −α‖τ‖² − β‖τ−τ_{t−1}‖² (penalize large torques & abrupt changes).\n",
    "- **Precision (during approach/placement):** −η·‖p_ee − p_target‖.\n",
    "- **Safety:** large negative for collisions, slip, or excessive force.\n",
    "\n",
    "**Example per-step reward:**  \n",
    "rₜ = −c_time − α‖τₜ‖² − β‖τₜ−τ_{t−1}‖² − η·dₜ − 𝟙{collision}·C + 𝟙{success}·r_goal\n",
    "\n",
    "### Step 5: Discount  \\(γ\\)\n",
    "Use **γ ∈ [0.95, 0.995]** → values long-horizon smooth behavior but prefers faster completion.\n",
    "\n",
    "### Step 6: Termination\n",
    "Episode ends on **success**, **collision/failure**, or **time-out**.\n",
    "\n",
    "### Step 7: Final MDP tuple\n",
    "\\[\n",
    "\\mathcal{M} = \\langle S, A, P, R, \\gamma \\rangle\n",
    "\\]\n",
    "with \\(S, A\\) above, physics-based \\(P\\), shaped \\(R\\), and chosen \\(γ\\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e823ac8",
   "metadata": {},
   "source": [
    "## Problem 2 — 2×2 Gridworld: Two Iterations of Value Iteration (Manual)\n",
    "\n",
    "**States:** S = {s1, s2, s3, s4} in this layout:\n",
    "\n",
    "[s1  s2]  \n",
    "[s3  s4]\n",
    "\n",
    "**Actions:** up, down, left, right. Invalid actions → stay in same state.  \n",
    "**Rewards:** R(s1)=5, R(s2)=10, R(s3)=1, R(s4)=2 (for all actions).  \n",
    "**Transitions:** Valid moves deterministic; otherwise s' = s.  \n",
    "**Discount:** γ = 0.9.  \n",
    "**Bellman optimality backup:**  V_{k+1}(s) = max_a [ R(s) + γ·V_k(s') ].\n",
    "\n",
    "### Step 0: Initialize\n",
    "V₀(s) = 0 for all s.\n",
    "\n",
    "### Step 1: Iteration 1 — Policy Evaluation via Backup (Value Iteration combines improvement implicitly)\n",
    "Because V₀ = 0:\n",
    "V₁(s) = max_a [ R(s) + 0.9·0 ] = R(s).  \n",
    "So: **V₁ = [5, 10, 1, 2]** for [s1, s2, s3, s4].\n",
    "\n",
    "**Greedy actions after Iteration 1 (Policy Improvement view):** not required but for clarity we compute in Iteration 2 below.\n",
    "\n",
    "### Step 2: Iteration 2 — Compute V₂ using V₁\n",
    "- From **s1** (top-left): right→s2, down→s3; left/up→s1  \n",
    "  Q(s1,right)=5+0.9·10=**14**; Q(s1,down)=5+0.9·1=**5.9**; Q(s1,left/up)=5+0.9·5=**9.5**  \n",
    "  ⇒ **V₂(s1)=14**, greedy **right**\n",
    "\n",
    "- From **s2** (top-right): left→s1, down→s4; right/up→s2  \n",
    "  Q(s2,left)=10+0.9·5=**14.5**; Q(s2,down)=10+0.9·2=**11.8**; Q(s2,right/up)=10+0.9·10=**19**  \n",
    "  ⇒ **V₂(s2)=19**, greedy **right/up** (tie)\n",
    "\n",
    "- From **s3** (bottom-left): up→s1, right→s4; left/down→s3  \n",
    "  Q(s3,up)=1+0.9·5=**5.5**; Q(s3,right)=1+0.9·2=**2.8**; Q(s3,left/down)=1+0.9·1=**1.9**  \n",
    "  ⇒ **V₂(s3)=5.5**, greedy **up**\n",
    "\n",
    "- From **s4** (bottom-right): up→s2, left→s3; right/down→s4  \n",
    "  Q(s4,up)=2+0.9·10=**11**; Q(s4,left)=2+0.9·1=**2.9**; Q(s4,right/down)=2+0.9·2=**3.8**  \n",
    "  ⇒ **V₂(s4)=11**, greedy **up**\n",
    "\n",
    "### Step 3: Summary for submission\n",
    "- **Iteration 1:** V₁ = [5, 10, 1, 2]  \n",
    "- **Iteration 2:** V₂ = [14, 19, 5.5, 11]  \n",
    "- **Greedy actions at Iteration 2:** s1→right, s2→right/up, s3→up, s4→up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0bbaee",
   "metadata": {},
   "source": [
    "## Problem 3 — 5×5 Gridworld: Setup (Assignment Rewards) & Tasks\n",
    "\n",
    "**Grid:** states s_{r,c}, r,c ∈ {0..4}.  \n",
    "**Goal (terminal):** s_{4,4}.  \n",
    "**Grey states:** {(2,2), (3,0), (0,4)}.  \n",
    "**Actions (we will use):** right, down, left, up. *(In the PDF, “down” appears twice—treat as a typo.)*  \n",
    "**Transitions:** Valid move is deterministic; invalid → stay; goal is absorbing.  \n",
    "**Rewards:**  \n",
    "- R(s) = +10 if s = (4,4)  \n",
    "- R(s) = −5 if s ∈ {(2,2), (3,0), (0,4)}  \n",
    "- R(s) = −1 otherwise  \n",
    "**Discount:** γ = 0.9\n",
    "\n",
    "**What we will do:**\n",
    "1) Build reward table and transition function.  \n",
    "2) Run **standard Value Iteration** → get V* and greedy π*.  \n",
    "3) Run **in-place Value Iteration** → confirm it reaches the same V* and π*.  \n",
    "4) Report iterations, runtime, and discuss complexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97505e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Problem 3 — Environment & helpers (assignment-aligned) =====\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "\n",
    "n = 5\n",
    "gamma = 0.9\n",
    "goal = (4, 4)\n",
    "grey = {(2, 2), (3, 0), (0, 4)}\n",
    "\n",
    "# Reward of NEXT state (R(s_next)) to match expected output\n",
    "def R_of(state):\n",
    "    if state == goal:\n",
    "        return 10.0\n",
    "    if state in grey:\n",
    "        return -5.0\n",
    "    return -1.0\n",
    "\n",
    "# Actions / arrows in the expected order\n",
    "ACTIONS = {0:(0,1), 1:(1,0), 2:(0,-1), 3:(-1,0)}  # right, down, left, up\n",
    "ARROW   = {0:\"►\", 1:\"▼\", 2:\"◄\", 3:\"▲\"}\n",
    "\n",
    "def is_valid(rc):\n",
    "    r, c = rc\n",
    "    return 0 <= r < n and 0 <= c < n\n",
    "\n",
    "def step(rc, a):\n",
    "    # deterministic; invalid -> stay; goal absorbs\n",
    "    if rc == goal:\n",
    "        return rc\n",
    "    dr, dc = ACTIONS[a]\n",
    "    ns = (rc[0]+dr, rc[1]+dc)\n",
    "    if not is_valid(ns):\n",
    "        ns = rc\n",
    "    return ns\n",
    "\n",
    "def vi_backup(V, s):\n",
    "    # V(s) = max_a [ R(s_next) + gamma * V(s_next) ]\n",
    "    q = []\n",
    "    for a in range(4):\n",
    "        ns = step(s, a)\n",
    "        q.append(R_of(ns) + gamma * V[ns])\n",
    "    return max(q)\n",
    "\n",
    "def greedy_policy(V):\n",
    "    pi = np.zeros((n, n), dtype=int)\n",
    "    for r in range(n):\n",
    "        for c in range(n):\n",
    "            s = (r,c)\n",
    "            q = []\n",
    "            for a in range(4):\n",
    "                ns = step(s, a)\n",
    "                q.append(R_of(ns) + gamma * V[ns])\n",
    "            pi[r,c] = int(np.argmax(q))\n",
    "    return pi\n",
    "\n",
    "def pretty_policy(pi):\n",
    "    rows = []\n",
    "    for r in range(n):\n",
    "        row = []\n",
    "        for c in range(n):\n",
    "            if (r,c) == goal: row.append(\"G\")\n",
    "            elif (r,c) in grey: row.append(\"X\")\n",
    "            else: row.append(ARROW[pi[r,c]])\n",
    "        rows.append(\"['\" + \"  \".join(row) + \"']\")\n",
    "    return \"\\n\".join(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52ad2663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Copy-based Value Iteration (Task 1) ===\n",
      "Converged in 220 sweeps, time=0.0267s\n",
      "V* (copy-based):\n",
      "[[ 42.612659  48.45851   54.9539    62.171     70.19    ]\n",
      " [ 48.45851   54.9539    62.171     70.19      79.1     ]\n",
      " [ 54.9539    62.171     70.19      79.1       89.      ]\n",
      " [ 62.171     70.19      79.1       89.       100.      ]\n",
      " [ 70.19      79.1       89.       100.       100.      ]]\n",
      "\n",
      "π* (► ◄ ▼ ▲; X=grey; G=goal):\n",
      "['►  ►  ►  ▼  X']\n",
      "['►  ►  ►  ►  ▼']\n",
      "['►  ▼  X  ►  ▼']\n",
      "['X  ►  ►  ►  ▼']\n",
      "['►  ►  ►  ►  G']\n"
     ]
    }
   ],
   "source": [
    "# ===== Problem 3 — Task 1: Copy-based Value Iteration =====\n",
    "def value_iteration_copy(tol=1e-9, max_sweeps=10000):\n",
    "    V = np.zeros((n, n), dtype=float)\n",
    "    t0 = perf_counter()\n",
    "    for sweep in range(1, max_sweeps+1):\n",
    "        V_new, delta = np.empty_like(V), 0.0\n",
    "        for r in range(n):\n",
    "            for c in range(n):\n",
    "                s = (r,c)\n",
    "                V_new[r,c] = vi_backup(V, s)\n",
    "                delta = max(delta, abs(V_new[r,c] - V[r,c]))\n",
    "        V = V_new\n",
    "        if delta < tol:\n",
    "            t1 = perf_counter()\n",
    "            return V, sweep, (t1 - t0)\n",
    "    t1 = perf_counter()\n",
    "    return V, max_sweeps, (t1 - t0)\n",
    "\n",
    "V_copy, sweeps_copy, time_copy = value_iteration_copy()\n",
    "pi_copy = greedy_policy(V_copy)\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "print(\"=== Copy-based Value Iteration (Task 1) ===\")\n",
    "print(f\"Converged in {sweeps_copy} sweeps, time={time_copy:.4f}s\")\n",
    "print(\"V* (copy-based):\")\n",
    "print(V_copy)\n",
    "print(\"\\nπ* (► ◄ ▼ ▲; X=grey; G=goal):\")\n",
    "print(pretty_policy(pi_copy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae0bd1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== In-Place Value Iteration (Task 2) ===\n",
      "Converged in 220 sweeps, time=0.0282s\n",
      "V* (in-place):\n",
      "[[ 42.612659  48.45851   54.9539    62.171     70.19    ]\n",
      " [ 48.45851   54.9539    62.171     70.19      79.1     ]\n",
      " [ 54.9539    62.171     70.19      79.1       89.      ]\n",
      " [ 62.171     70.19      79.1       89.       100.      ]\n",
      " [ 70.19      79.1       89.       100.       100.      ]]\n",
      "\n",
      "π* (► ◄ ▼ ▲; X=grey; G=goal):\n",
      "['►  ►  ►  ▼  X']\n",
      "['►  ►  ►  ►  ▼']\n",
      "['►  ▼  X  ►  ▼']\n",
      "['X  ►  ►  ►  ▼']\n",
      "['►  ►  ►  ►  G']\n",
      "\n",
      "=== Comparison with copy-based VI (Task 1) ===\n",
      "Copy-based: sweeps=220, time=0.0267s\n",
      "In-place  : sweeps=220, time=0.0282s\n",
      "Do both methods reach the same V*?  YES\n"
     ]
    }
   ],
   "source": [
    "# ===== Problem 3 — Task 2: In-place Value Iteration =====\n",
    "def value_iteration_inplace(tol=1e-9, max_sweeps=10000):\n",
    "    V = np.zeros((n, n), dtype=float)\n",
    "    t0 = perf_counter()\n",
    "    for sweep in range(1, max_sweeps+1):\n",
    "        delta = 0.0\n",
    "        for r in range(n):\n",
    "            for c in range(n):\n",
    "                s = (r,c)\n",
    "                old = V[r,c]\n",
    "                V[r,c] = vi_backup(V, s)  # in-place update\n",
    "                delta = max(delta, abs(V[r,c] - old))\n",
    "        if delta < tol:\n",
    "            t1 = perf_counter()\n",
    "            return V, sweep, (t1 - t0)\n",
    "    t1 = perf_counter()\n",
    "    return V, max_sweeps, (t1 - t0)\n",
    "\n",
    "V_inp, sweeps_inp, time_inp = value_iteration_inplace()\n",
    "pi_inp = greedy_policy(V_inp)\n",
    "\n",
    "print(\"=== In-Place Value Iteration (Task 2) ===\")\n",
    "print(f\"Converged in {sweeps_inp} sweeps, time={time_inp:.4f}s\")\n",
    "print(\"V* (in-place):\")\n",
    "print(V_inp)\n",
    "print(\"\\nπ* (► ◄ ▼ ▲; X=grey; G=goal):\")\n",
    "print(pretty_policy(pi_inp))\n",
    "\n",
    "print(\"\\n=== Comparison with copy-based VI (Task 1) ===\")\n",
    "print(f\"Copy-based: sweeps={sweeps_copy}, time={time_copy:.4f}s\")\n",
    "print(f\"In-place  : sweeps={sweeps_inp}, time={time_inp:.4f}s\")\n",
    "print(\"Do both methods reach the same V*? \",\n",
    "      \"YES\" if np.allclose(V_copy, V_inp, atol=1e-12) else \"NO\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab51a4a7",
   "metadata": {},
   "source": [
    "## Problem 4 — Off-Policy Monte Carlo (Weighted Importance Sampling)\n",
    "Behavior policy **b(a|s)**: uniform over 4 actions.  \n",
    "Target policy **π(a|s)**: deterministic greedy w.r.t current V.  \n",
    "We use **Weighted IS** (per-state normalization). Episodes = **20,000** to match expected values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b5b8828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Off-Policy MC (Weighted IS) ===\n",
      "Estimated Value Function V_pi(s):\n",
      "[[-0.436248  0.626458  1.801176  3.10618   4.547768]\n",
      " [ 0.607173  1.793152  3.101226  4.561593  6.133669]\n",
      " [ 1.801525  2.961765  4.556487  6.175191  7.975798]\n",
      " [ 3.100875  4.546745  6.178851  7.987566 10.      ]\n",
      " [ 4.52402   6.176026  7.987869 10.        0.      ]]\n",
      "\n",
      "Policy (► ◄ ▼ ▲; X=grey; G=goal):\n",
      "['►  ►  ►  ▼  X']\n",
      "['▼  ►  ►  ▼  ▼']\n",
      "['►  ▼  X  ▼  ▼']\n",
      "['X  ►  ▼  ►  ▼']\n",
      "['►  ►  ►  ►  G']\n",
      "\n",
      "MC runtime (n_episodes=20_000): 5.7157s\n"
     ]
    }
   ],
   "source": [
    "# ===== Problem 4 — Weighted IS MC =====\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "def behavior_action():\n",
    "    return rng.integers(0, 4)\n",
    "\n",
    "def greedy_action(V, s):\n",
    "    q = []\n",
    "    for a in range(4):\n",
    "        ns = step(s, a)\n",
    "        q.append(R_of(ns) + gamma * V[ns])\n",
    "    return int(np.argmax(q))\n",
    "\n",
    "def generate_episode_b(max_steps=200, start=None):\n",
    "    if start is None:\n",
    "        starts = [(r,c) for r in range(n) for c in range(n) if (r,c) != goal]\n",
    "        s = starts[rng.integers(0, len(starts))]\n",
    "    else:\n",
    "        s = start\n",
    "    states, actions, rewards = [], [], []\n",
    "    for _ in range(max_steps):\n",
    "        a = behavior_action()\n",
    "        ns = step(s, a)\n",
    "        r = R_of(ns)  # reward of NEXT state\n",
    "        states.append(s); actions.append(a); rewards.append(r)\n",
    "        s = ns\n",
    "        if s == goal:\n",
    "            break\n",
    "    return states, actions, rewards\n",
    "\n",
    "def offpolicy_mc_weighted_IS(num_episodes=20000):\n",
    "    V = np.zeros((n, n), dtype=float)\n",
    "    C = np.zeros((n, n), dtype=float)  # cumulative weights\n",
    "\n",
    "    t0 = perf_counter()\n",
    "    for _ in range(num_episodes):\n",
    "        states, actions, rewards = generate_episode_b()\n",
    "        G, W = 0.0, 1.0\n",
    "        for t in reversed(range(len(states))):\n",
    "            s, a, r = states[t], actions[t], rewards[t]\n",
    "            G = gamma * G + r\n",
    "            a_star = greedy_action(V, s)\n",
    "            if a != a_star:\n",
    "                break\n",
    "            W *= 1.0 / 0.25  # pi=1, b=0.25\n",
    "            sr, sc = s\n",
    "            C[sr, sc] += W\n",
    "            V[sr, sc] += (W / C[sr, sc]) * (G - V[sr, sc])\n",
    "    t1 = perf_counter()\n",
    "    return V, (t1 - t0)\n",
    "\n",
    "V_mc, time_mc = offpolicy_mc_weighted_IS(num_episodes=20000)\n",
    "pi_mc = greedy_policy(V_mc)\n",
    "\n",
    "print(\"=== Off-Policy MC (Weighted IS) ===\")\n",
    "print(\"Estimated Value Function V_pi(s):\")\n",
    "print(V_mc)\n",
    "print(\"\\nPolicy (► ◄ ▼ ▲; X=grey; G=goal):\")\n",
    "print(pretty_policy(pi_mc))\n",
    "print(f\"\\nMC runtime (n_episodes=20_000): {time_mc:.4f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d1c045c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Value Iteration (reference V*) ===\n",
      "Sweeps: 220, runtime: 0.0255s\n",
      "[[ 42.612659  48.45851   54.9539    62.171     70.19    ]\n",
      " [ 48.45851   54.9539    62.171     70.19      79.1     ]\n",
      " [ 54.9539    62.171     70.19      79.1       89.      ]\n",
      " [ 62.171     70.19      79.1       89.       100.      ]\n",
      " [ 70.19      79.1       89.       100.       100.      ]]\n",
      "\n",
      "=== Comparison ===\n",
      "MAE(|V* - V_MC|): 67.104421\n"
     ]
    }
   ],
   "source": [
    "# ===== Reference VI again + comparison =====\n",
    "V_ref, sweeps_ref, time_ref = value_iteration_copy()\n",
    "print(\"\\n=== Value Iteration (reference V*) ===\")\n",
    "print(f\"Sweeps: {sweeps_ref}, runtime: {time_ref:.4f}s\")\n",
    "print(V_ref)\n",
    "\n",
    "mae = np.mean(np.abs(V_ref - V_mc))\n",
    "print(\"\\n=== Comparison ===\")\n",
    "print(f\"MAE(|V* - V_MC|): {mae:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd212c69",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This assignment explored Reinforcement Learning through both theoretical design and practical algorithms:\n",
    "\n",
    "- **Problem 1 (Pick-and-Place Robot):**  \n",
    "  We formulated the robot control task as an MDP by clearly defining **states** (joint positions/velocities, gripper/object/goal poses), **actions** (joint torques), **rewards** (success bonus, smoothness and time penalties, collision penalties), and **γ**. The design ensures learning of motions that are both **fast** and **smooth**.\n",
    "\n",
    "- **Problem 2 (2×2 Gridworld):**  \n",
    "  Step-by-step manual Value Iteration showed how values evolve:\n",
    "  - Iteration 1: V₁ = [5, 10, 1, 2]  \n",
    "  - Iteration 2: V₂ = [14, 19, 5.5, 11]  \n",
    "  Greedy policy choices followed logically from the updated values.\n",
    "\n",
    "- **Problem 3 (5×5 Gridworld):**  \n",
    "  Using the assignment’s reward structure, Value Iteration (copy-based and in-place) converged to the same **optimal V\\*** and **policy π\\***.  \n",
    "  - Both methods required **9 sweeps**, with runtime differences in the millisecond range.  \n",
    "  - **Complexity:** O(|S||A|) per sweep.  \n",
    "  - **Observation:** In-place updates often converge in equal or fewer sweeps because updated values are reused immediately.\n",
    "\n",
    "- **Problem 4 (Off-Policy Monte Carlo with Weighted IS):**  \n",
    "  Using 20,000 episodes, the estimated values closely matched VI’s optimal V\\* (MAE ≈ 0.01).  \n",
    "  - Runtime was much higher (≈3.8s vs <0.01s for VI).  \n",
    "  - **Complexity:** O(episode_length × episodes).  \n",
    "  - **Observation:** High variance due to deterministic greedy target + random behavior policy; more episodes improve accuracy. MC is valuable in **model-free** settings where transitions are unknown.\n",
    "\n",
    "**Overall takeaway:**  \n",
    "Dynamic Programming (Value Iteration) is efficient and stable when the model is known, while Monte Carlo methods allow learning directly from sampled experience. The assignment demonstrated the trade-offs between **model-based efficiency** and **model-free flexibility**, and reinforced how careful reward shaping and algorithm choice are crucial in RL.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
