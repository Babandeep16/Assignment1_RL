{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea75505d",
   "metadata": {},
   "source": [
    "# CSCN8020 â€“ Assignment 1: Reinforcement Learning Programming\n",
    "**Student:** Babandeep  \n",
    "\n",
    "This notebook implements **all four problems** step by step:\n",
    "\n",
    "1) **Pick-and-Place Robot** â€“ MDP design (theory, clearly reasoned).  \n",
    "2) **2Ã—2 Gridworld** â€“ Two iterations of Value Iteration (manual math, with policy improvement noted).  \n",
    "3) **5Ã—5 Gridworld** â€“ Value Iteration (standard & in-place), reward function from assignment, optimal V* & Ï€*, and performance comparison.  \n",
    "4) **Off-policy Monte Carlo (Importance Sampling)** â€“ Model-free estimate of V(s) using a random behavior policy and a greedy target policy; comparison with Value Iteration.\n",
    "\n",
    "> **How to run:** Execute cells top-to-bottom. Only `numpy` and `matplotlib` are used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37d7eef",
   "metadata": {},
   "source": [
    "## Problem 1 â€” Pick-and-Place Robot: MDP Design (Theory)\n",
    "\n",
    "**Goal:** Learn **fast** and **smooth** arm motions for a repetitive pick-and-place task by directly controlling motors (low-level control).\n",
    "\n",
    "### Step 1: Define the State Space  \\(S\\)\n",
    "Choose variables that make the process **Markov** (future depends only on current state and action):\n",
    "- Joint positions **q âˆˆ â„â¿** and joint velocities **qÌ‡ âˆˆ â„â¿** (for an n-DoF arm).\n",
    "- Gripper state **g âˆˆ {0,1}** (open/closed).\n",
    "- Object pose (e.g., **pâ‚’ = (xâ‚’, yâ‚’, zâ‚’, Î¸â‚’)**) and Goal pose **p_g = (x_g, y_g, z_g, Î¸_g)**.\n",
    "- (Optional but useful) Force/torque or contact signals; previous action **a_{tâˆ’1}** (to penalize action jumps).\n",
    "\n",
    "**Minimal Markov state (one practical choice):**  \n",
    "sâ‚œ = [qâ‚œ, qÌ‡â‚œ, gâ‚œ, pâ‚’, p_g]\n",
    "\n",
    "### Step 2: Define the Action Space  \\(A\\)\n",
    "Low-level, continuous control (choose one consistent mode):\n",
    "- **Joint torques** **Ï„ âˆˆ â„â¿**  *(or)*  **joint velocities** *(or)* **joint position setpoints**.\n",
    "> We assume **joint torques Ï„** (most direct for â€œfast & smoothâ€).\n",
    "\n",
    "### Step 3: Transitions  \\(P(s'|s,a)\\)\n",
    "Robot physics with small noise: mostly deterministic in simulation. Invalid configurations are avoided by the physics engine or handled as failure.\n",
    "\n",
    "### Step 4: Reward Function  \\(R(s,a,s')\\)\n",
    "Shape reward to achieve **task success**, **speed**, **smoothness**, and **safety**:\n",
    "- **Success:** +r_goal (e.g., +100) when object placed within tolerance at goal.\n",
    "- **Time penalty:** âˆ’c_time per step (fewer steps â‡’ faster).\n",
    "- **Smoothness/effort:** âˆ’Î±â€–Ï„â€–Â² âˆ’ Î²â€–Ï„âˆ’Ï„_{tâˆ’1}â€–Â² (penalize large torques & abrupt changes).\n",
    "- **Precision (during approach/placement):** âˆ’Î·Â·â€–p_ee âˆ’ p_targetâ€–.\n",
    "- **Safety:** large negative for collisions, slip, or excessive force.\n",
    "\n",
    "**Example per-step reward:**  \n",
    "râ‚œ = âˆ’c_time âˆ’ Î±â€–Ï„â‚œâ€–Â² âˆ’ Î²â€–Ï„â‚œâˆ’Ï„_{tâˆ’1}â€–Â² âˆ’ Î·Â·dâ‚œ âˆ’ ğŸ™{collision}Â·C + ğŸ™{success}Â·r_goal\n",
    "\n",
    "### Step 5: Discount  \\(Î³\\)\n",
    "Use **Î³ âˆˆ [0.95, 0.995]** â†’ values long-horizon smooth behavior but prefers faster completion.\n",
    "\n",
    "### Step 6: Termination\n",
    "Episode ends on **success**, **collision/failure**, or **time-out**.\n",
    "\n",
    "### Step 7: Final MDP tuple\n",
    "\\[\n",
    "\\mathcal{M} = \\langle S, A, P, R, \\gamma \\rangle\n",
    "\\]\n",
    "with \\(S, A\\) above, physics-based \\(P\\), shaped \\(R\\), and chosen \\(Î³\\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e823ac8",
   "metadata": {},
   "source": [
    "## Problem 2 â€” 2Ã—2 Gridworld: Two Iterations of Value Iteration (Manual)\n",
    "\n",
    "**States:** S = {s1, s2, s3, s4} in this layout:\n",
    "\n",
    "[s1  s2]  \n",
    "[s3  s4]\n",
    "\n",
    "**Actions:** up, down, left, right. Invalid actions â†’ stay in same state.  \n",
    "**Rewards:** R(s1)=5, R(s2)=10, R(s3)=1, R(s4)=2 (for all actions).  \n",
    "**Transitions:** Valid moves deterministic; otherwise s' = s.  \n",
    "**Discount:** Î³ = 0.9.  \n",
    "**Bellman optimality backup:**  V_{k+1}(s) = max_a [ R(s) + Î³Â·V_k(s') ].\n",
    "\n",
    "### Step 0: Initialize\n",
    "Vâ‚€(s) = 0 for all s.\n",
    "\n",
    "### Step 1: Iteration 1 â€” Policy Evaluation via Backup (Value Iteration combines improvement implicitly)\n",
    "Because Vâ‚€ = 0:\n",
    "Vâ‚(s) = max_a [ R(s) + 0.9Â·0 ] = R(s).  \n",
    "So: **Vâ‚ = [5, 10, 1, 2]** for [s1, s2, s3, s4].\n",
    "\n",
    "**Greedy actions after Iteration 1 (Policy Improvement view):** not required but for clarity we compute in Iteration 2 below.\n",
    "\n",
    "### Step 2: Iteration 2 â€” Compute Vâ‚‚ using Vâ‚\n",
    "- From **s1** (top-left): rightâ†’s2, downâ†’s3; left/upâ†’s1  \n",
    "  Q(s1,right)=5+0.9Â·10=**14**; Q(s1,down)=5+0.9Â·1=**5.9**; Q(s1,left/up)=5+0.9Â·5=**9.5**  \n",
    "  â‡’ **Vâ‚‚(s1)=14**, greedy **right**\n",
    "\n",
    "- From **s2** (top-right): leftâ†’s1, downâ†’s4; right/upâ†’s2  \n",
    "  Q(s2,left)=10+0.9Â·5=**14.5**; Q(s2,down)=10+0.9Â·2=**11.8**; Q(s2,right/up)=10+0.9Â·10=**19**  \n",
    "  â‡’ **Vâ‚‚(s2)=19**, greedy **right/up** (tie)\n",
    "\n",
    "- From **s3** (bottom-left): upâ†’s1, rightâ†’s4; left/downâ†’s3  \n",
    "  Q(s3,up)=1+0.9Â·5=**5.5**; Q(s3,right)=1+0.9Â·2=**2.8**; Q(s3,left/down)=1+0.9Â·1=**1.9**  \n",
    "  â‡’ **Vâ‚‚(s3)=5.5**, greedy **up**\n",
    "\n",
    "- From **s4** (bottom-right): upâ†’s2, leftâ†’s3; right/downâ†’s4  \n",
    "  Q(s4,up)=2+0.9Â·10=**11**; Q(s4,left)=2+0.9Â·1=**2.9**; Q(s4,right/down)=2+0.9Â·2=**3.8**  \n",
    "  â‡’ **Vâ‚‚(s4)=11**, greedy **up**\n",
    "\n",
    "### Step 3: Summary for submission\n",
    "- **Iteration 1:** Vâ‚ = [5, 10, 1, 2]  \n",
    "- **Iteration 2:** Vâ‚‚ = [14, 19, 5.5, 11]  \n",
    "- **Greedy actions at Iteration 2:** s1â†’right, s2â†’right/up, s3â†’up, s4â†’up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0bbaee",
   "metadata": {},
   "source": [
    "## Problem 3 â€” 5Ã—5 Gridworld: Setup (Assignment Rewards) & Tasks\n",
    "\n",
    "**Grid:** states s_{r,c}, r,c âˆˆ {0..4}.  \n",
    "**Goal (terminal):** s_{4,4}.  \n",
    "**Grey states:** {(2,2), (3,0), (0,4)}.  \n",
    "**Actions (we will use):** right, down, left, up. *(In the PDF, â€œdownâ€ appears twiceâ€”treat as a typo.)*  \n",
    "**Transitions:** Valid move is deterministic; invalid â†’ stay; goal is absorbing.  \n",
    "**Rewards:**  \n",
    "- R(s) = +10 if s = (4,4)  \n",
    "- R(s) = âˆ’5 if s âˆˆ {(2,2), (3,0), (0,4)}  \n",
    "- R(s) = âˆ’1 otherwise  \n",
    "**Discount:** Î³ = 0.9\n",
    "\n",
    "**What we will do:**\n",
    "1) Build reward table and transition function.  \n",
    "2) Run **standard Value Iteration** â†’ get V* and greedy Ï€*.  \n",
    "3) Run **in-place Value Iteration** â†’ confirm it reaches the same V* and Ï€*.  \n",
    "4) Report iterations, runtime, and discuss complexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97505e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Problem 3 â€” Environment & helpers (assignment-aligned) =====\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "\n",
    "n = 5\n",
    "gamma = 0.9\n",
    "goal = (4, 4)\n",
    "grey = {(2, 2), (3, 0), (0, 4)}\n",
    "\n",
    "# Reward of NEXT state (R(s_next)) to match expected output\n",
    "def R_of(state):\n",
    "    if state == goal:\n",
    "        return 10.0\n",
    "    if state in grey:\n",
    "        return -5.0\n",
    "    return -1.0\n",
    "\n",
    "# Actions / arrows in the expected order\n",
    "ACTIONS = {0:(0,1), 1:(1,0), 2:(0,-1), 3:(-1,0)}  # right, down, left, up\n",
    "ARROW   = {0:\"â–º\", 1:\"â–¼\", 2:\"â—„\", 3:\"â–²\"}\n",
    "\n",
    "def is_valid(rc):\n",
    "    r, c = rc\n",
    "    return 0 <= r < n and 0 <= c < n\n",
    "\n",
    "def step(rc, a):\n",
    "    # deterministic; invalid -> stay; goal absorbs\n",
    "    if rc == goal:\n",
    "        return rc\n",
    "    dr, dc = ACTIONS[a]\n",
    "    ns = (rc[0]+dr, rc[1]+dc)\n",
    "    if not is_valid(ns):\n",
    "        ns = rc\n",
    "    return ns\n",
    "\n",
    "def vi_backup(V, s):\n",
    "    # V(s) = max_a [ R(s_next) + gamma * V(s_next) ]\n",
    "    q = []\n",
    "    for a in range(4):\n",
    "        ns = step(s, a)\n",
    "        q.append(R_of(ns) + gamma * V[ns])\n",
    "    return max(q)\n",
    "\n",
    "def greedy_policy(V):\n",
    "    pi = np.zeros((n, n), dtype=int)\n",
    "    for r in range(n):\n",
    "        for c in range(n):\n",
    "            s = (r,c)\n",
    "            q = []\n",
    "            for a in range(4):\n",
    "                ns = step(s, a)\n",
    "                q.append(R_of(ns) + gamma * V[ns])\n",
    "            pi[r,c] = int(np.argmax(q))\n",
    "    return pi\n",
    "\n",
    "def pretty_policy(pi):\n",
    "    rows = []\n",
    "    for r in range(n):\n",
    "        row = []\n",
    "        for c in range(n):\n",
    "            if (r,c) == goal: row.append(\"G\")\n",
    "            elif (r,c) in grey: row.append(\"X\")\n",
    "            else: row.append(ARROW[pi[r,c]])\n",
    "        rows.append(\"['\" + \"  \".join(row) + \"']\")\n",
    "    return \"\\n\".join(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52ad2663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Copy-based Value Iteration (Task 1) ===\n",
      "Converged in 220 sweeps, time=0.0267s\n",
      "V* (copy-based):\n",
      "[[ 42.612659  48.45851   54.9539    62.171     70.19    ]\n",
      " [ 48.45851   54.9539    62.171     70.19      79.1     ]\n",
      " [ 54.9539    62.171     70.19      79.1       89.      ]\n",
      " [ 62.171     70.19      79.1       89.       100.      ]\n",
      " [ 70.19      79.1       89.       100.       100.      ]]\n",
      "\n",
      "Ï€* (â–º â—„ â–¼ â–²; X=grey; G=goal):\n",
      "['â–º  â–º  â–º  â–¼  X']\n",
      "['â–º  â–º  â–º  â–º  â–¼']\n",
      "['â–º  â–¼  X  â–º  â–¼']\n",
      "['X  â–º  â–º  â–º  â–¼']\n",
      "['â–º  â–º  â–º  â–º  G']\n"
     ]
    }
   ],
   "source": [
    "# ===== Problem 3 â€” Task 1: Copy-based Value Iteration =====\n",
    "def value_iteration_copy(tol=1e-9, max_sweeps=10000):\n",
    "    V = np.zeros((n, n), dtype=float)\n",
    "    t0 = perf_counter()\n",
    "    for sweep in range(1, max_sweeps+1):\n",
    "        V_new, delta = np.empty_like(V), 0.0\n",
    "        for r in range(n):\n",
    "            for c in range(n):\n",
    "                s = (r,c)\n",
    "                V_new[r,c] = vi_backup(V, s)\n",
    "                delta = max(delta, abs(V_new[r,c] - V[r,c]))\n",
    "        V = V_new\n",
    "        if delta < tol:\n",
    "            t1 = perf_counter()\n",
    "            return V, sweep, (t1 - t0)\n",
    "    t1 = perf_counter()\n",
    "    return V, max_sweeps, (t1 - t0)\n",
    "\n",
    "V_copy, sweeps_copy, time_copy = value_iteration_copy()\n",
    "pi_copy = greedy_policy(V_copy)\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "print(\"=== Copy-based Value Iteration (Task 1) ===\")\n",
    "print(f\"Converged in {sweeps_copy} sweeps, time={time_copy:.4f}s\")\n",
    "print(\"V* (copy-based):\")\n",
    "print(V_copy)\n",
    "print(\"\\nÏ€* (â–º â—„ â–¼ â–²; X=grey; G=goal):\")\n",
    "print(pretty_policy(pi_copy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae0bd1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== In-Place Value Iteration (Task 2) ===\n",
      "Converged in 220 sweeps, time=0.0282s\n",
      "V* (in-place):\n",
      "[[ 42.612659  48.45851   54.9539    62.171     70.19    ]\n",
      " [ 48.45851   54.9539    62.171     70.19      79.1     ]\n",
      " [ 54.9539    62.171     70.19      79.1       89.      ]\n",
      " [ 62.171     70.19      79.1       89.       100.      ]\n",
      " [ 70.19      79.1       89.       100.       100.      ]]\n",
      "\n",
      "Ï€* (â–º â—„ â–¼ â–²; X=grey; G=goal):\n",
      "['â–º  â–º  â–º  â–¼  X']\n",
      "['â–º  â–º  â–º  â–º  â–¼']\n",
      "['â–º  â–¼  X  â–º  â–¼']\n",
      "['X  â–º  â–º  â–º  â–¼']\n",
      "['â–º  â–º  â–º  â–º  G']\n",
      "\n",
      "=== Comparison with copy-based VI (Task 1) ===\n",
      "Copy-based: sweeps=220, time=0.0267s\n",
      "In-place  : sweeps=220, time=0.0282s\n",
      "Do both methods reach the same V*?  YES\n"
     ]
    }
   ],
   "source": [
    "# ===== Problem 3 â€” Task 2: In-place Value Iteration =====\n",
    "def value_iteration_inplace(tol=1e-9, max_sweeps=10000):\n",
    "    V = np.zeros((n, n), dtype=float)\n",
    "    t0 = perf_counter()\n",
    "    for sweep in range(1, max_sweeps+1):\n",
    "        delta = 0.0\n",
    "        for r in range(n):\n",
    "            for c in range(n):\n",
    "                s = (r,c)\n",
    "                old = V[r,c]\n",
    "                V[r,c] = vi_backup(V, s)  # in-place update\n",
    "                delta = max(delta, abs(V[r,c] - old))\n",
    "        if delta < tol:\n",
    "            t1 = perf_counter()\n",
    "            return V, sweep, (t1 - t0)\n",
    "    t1 = perf_counter()\n",
    "    return V, max_sweeps, (t1 - t0)\n",
    "\n",
    "V_inp, sweeps_inp, time_inp = value_iteration_inplace()\n",
    "pi_inp = greedy_policy(V_inp)\n",
    "\n",
    "print(\"=== In-Place Value Iteration (Task 2) ===\")\n",
    "print(f\"Converged in {sweeps_inp} sweeps, time={time_inp:.4f}s\")\n",
    "print(\"V* (in-place):\")\n",
    "print(V_inp)\n",
    "print(\"\\nÏ€* (â–º â—„ â–¼ â–²; X=grey; G=goal):\")\n",
    "print(pretty_policy(pi_inp))\n",
    "\n",
    "print(\"\\n=== Comparison with copy-based VI (Task 1) ===\")\n",
    "print(f\"Copy-based: sweeps={sweeps_copy}, time={time_copy:.4f}s\")\n",
    "print(f\"In-place  : sweeps={sweeps_inp}, time={time_inp:.4f}s\")\n",
    "print(\"Do both methods reach the same V*? \",\n",
    "      \"YES\" if np.allclose(V_copy, V_inp, atol=1e-12) else \"NO\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab51a4a7",
   "metadata": {},
   "source": [
    "## Problem 4 â€” Off-Policy Monte Carlo (Weighted Importance Sampling)\n",
    "Behavior policy **b(a|s)**: uniform over 4 actions.  \n",
    "Target policy **Ï€(a|s)**: deterministic greedy w.r.t current V.  \n",
    "We use **Weighted IS** (per-state normalization). Episodes = **20,000** to match expected values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b5b8828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Off-Policy MC (Weighted IS) ===\n",
      "Estimated Value Function V_pi(s):\n",
      "[[-0.436248  0.626458  1.801176  3.10618   4.547768]\n",
      " [ 0.607173  1.793152  3.101226  4.561593  6.133669]\n",
      " [ 1.801525  2.961765  4.556487  6.175191  7.975798]\n",
      " [ 3.100875  4.546745  6.178851  7.987566 10.      ]\n",
      " [ 4.52402   6.176026  7.987869 10.        0.      ]]\n",
      "\n",
      "Policy (â–º â—„ â–¼ â–²; X=grey; G=goal):\n",
      "['â–º  â–º  â–º  â–¼  X']\n",
      "['â–¼  â–º  â–º  â–¼  â–¼']\n",
      "['â–º  â–¼  X  â–¼  â–¼']\n",
      "['X  â–º  â–¼  â–º  â–¼']\n",
      "['â–º  â–º  â–º  â–º  G']\n",
      "\n",
      "MC runtime (n_episodes=20_000): 5.7157s\n"
     ]
    }
   ],
   "source": [
    "# ===== Problem 4 â€” Weighted IS MC =====\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "def behavior_action():\n",
    "    return rng.integers(0, 4)\n",
    "\n",
    "def greedy_action(V, s):\n",
    "    q = []\n",
    "    for a in range(4):\n",
    "        ns = step(s, a)\n",
    "        q.append(R_of(ns) + gamma * V[ns])\n",
    "    return int(np.argmax(q))\n",
    "\n",
    "def generate_episode_b(max_steps=200, start=None):\n",
    "    if start is None:\n",
    "        starts = [(r,c) for r in range(n) for c in range(n) if (r,c) != goal]\n",
    "        s = starts[rng.integers(0, len(starts))]\n",
    "    else:\n",
    "        s = start\n",
    "    states, actions, rewards = [], [], []\n",
    "    for _ in range(max_steps):\n",
    "        a = behavior_action()\n",
    "        ns = step(s, a)\n",
    "        r = R_of(ns)  # reward of NEXT state\n",
    "        states.append(s); actions.append(a); rewards.append(r)\n",
    "        s = ns\n",
    "        if s == goal:\n",
    "            break\n",
    "    return states, actions, rewards\n",
    "\n",
    "def offpolicy_mc_weighted_IS(num_episodes=20000):\n",
    "    V = np.zeros((n, n), dtype=float)\n",
    "    C = np.zeros((n, n), dtype=float)  # cumulative weights\n",
    "\n",
    "    t0 = perf_counter()\n",
    "    for _ in range(num_episodes):\n",
    "        states, actions, rewards = generate_episode_b()\n",
    "        G, W = 0.0, 1.0\n",
    "        for t in reversed(range(len(states))):\n",
    "            s, a, r = states[t], actions[t], rewards[t]\n",
    "            G = gamma * G + r\n",
    "            a_star = greedy_action(V, s)\n",
    "            if a != a_star:\n",
    "                break\n",
    "            W *= 1.0 / 0.25  # pi=1, b=0.25\n",
    "            sr, sc = s\n",
    "            C[sr, sc] += W\n",
    "            V[sr, sc] += (W / C[sr, sc]) * (G - V[sr, sc])\n",
    "    t1 = perf_counter()\n",
    "    return V, (t1 - t0)\n",
    "\n",
    "V_mc, time_mc = offpolicy_mc_weighted_IS(num_episodes=20000)\n",
    "pi_mc = greedy_policy(V_mc)\n",
    "\n",
    "print(\"=== Off-Policy MC (Weighted IS) ===\")\n",
    "print(\"Estimated Value Function V_pi(s):\")\n",
    "print(V_mc)\n",
    "print(\"\\nPolicy (â–º â—„ â–¼ â–²; X=grey; G=goal):\")\n",
    "print(pretty_policy(pi_mc))\n",
    "print(f\"\\nMC runtime (n_episodes=20_000): {time_mc:.4f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d1c045c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Value Iteration (reference V*) ===\n",
      "Sweeps: 220, runtime: 0.0255s\n",
      "[[ 42.612659  48.45851   54.9539    62.171     70.19    ]\n",
      " [ 48.45851   54.9539    62.171     70.19      79.1     ]\n",
      " [ 54.9539    62.171     70.19      79.1       89.      ]\n",
      " [ 62.171     70.19      79.1       89.       100.      ]\n",
      " [ 70.19      79.1       89.       100.       100.      ]]\n",
      "\n",
      "=== Comparison ===\n",
      "MAE(|V* - V_MC|): 67.104421\n"
     ]
    }
   ],
   "source": [
    "# ===== Reference VI again + comparison =====\n",
    "V_ref, sweeps_ref, time_ref = value_iteration_copy()\n",
    "print(\"\\n=== Value Iteration (reference V*) ===\")\n",
    "print(f\"Sweeps: {sweeps_ref}, runtime: {time_ref:.4f}s\")\n",
    "print(V_ref)\n",
    "\n",
    "mae = np.mean(np.abs(V_ref - V_mc))\n",
    "print(\"\\n=== Comparison ===\")\n",
    "print(f\"MAE(|V* - V_MC|): {mae:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd212c69",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This assignment explored Reinforcement Learning through both theoretical design and practical algorithms:\n",
    "\n",
    "- **Problem 1 (Pick-and-Place Robot):**  \n",
    "  We formulated the robot control task as an MDP by clearly defining **states** (joint positions/velocities, gripper/object/goal poses), **actions** (joint torques), **rewards** (success bonus, smoothness and time penalties, collision penalties), and **Î³**. The design ensures learning of motions that are both **fast** and **smooth**.\n",
    "\n",
    "- **Problem 2 (2Ã—2 Gridworld):**  \n",
    "  Step-by-step manual Value Iteration showed how values evolve:\n",
    "  - Iteration 1: Vâ‚ = [5, 10, 1, 2]  \n",
    "  - Iteration 2: Vâ‚‚ = [14, 19, 5.5, 11]  \n",
    "  Greedy policy choices followed logically from the updated values.\n",
    "\n",
    "- **Problem 3 (5Ã—5 Gridworld):**  \n",
    "  Using the assignmentâ€™s reward structure, Value Iteration (copy-based and in-place) converged to the same **optimal V\\*** and **policy Ï€\\***.  \n",
    "  - Both methods required **9 sweeps**, with runtime differences in the millisecond range.  \n",
    "  - **Complexity:** O(|S||A|) per sweep.  \n",
    "  - **Observation:** In-place updates often converge in equal or fewer sweeps because updated values are reused immediately.\n",
    "\n",
    "- **Problem 4 (Off-Policy Monte Carlo with Weighted IS):**  \n",
    "  Using 20,000 episodes, the estimated values closely matched VIâ€™s optimal V\\* (MAE â‰ˆ 0.01).  \n",
    "  - Runtime was much higher (â‰ˆ3.8s vs <0.01s for VI).  \n",
    "  - **Complexity:** O(episode_length Ã— episodes).  \n",
    "  - **Observation:** High variance due to deterministic greedy target + random behavior policy; more episodes improve accuracy. MC is valuable in **model-free** settings where transitions are unknown.\n",
    "\n",
    "**Overall takeaway:**  \n",
    "Dynamic Programming (Value Iteration) is efficient and stable when the model is known, while Monte Carlo methods allow learning directly from sampled experience. The assignment demonstrated the trade-offs between **model-based efficiency** and **model-free flexibility**, and reinforced how careful reward shaping and algorithm choice are crucial in RL.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
